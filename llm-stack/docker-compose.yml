services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    restart: unless-stopped

  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_storage:/qdrant/storage
    restart: unless-stopped

  anythingllm:
    image: mintplexlabs/anythingllm:latest
    container_name: anythingllm
    ports:
      - "3001:3001"
    environment:
          # Core settings
      - NODE_ENV=production
      - STORAGE_DIR=/app/server/storage
      - FILE_UPLOAD_SIZE_LIMIT=100

      # Use Ollama in OpenAI-compatible mode
      - LLM_PROVIDER=openai
      - OPENAI_BASE_PATH=http://ollama:11434/v1

      # Dummy API key â€” required by AnythingLLM, ignored by Ollama
      - OPENAI_API_KEY=local-ollama

      # Vector DB
      - VECTOR_DB=qdrant
      - QDRANT_URL=http://qdrant:6333

      # Needed for browser/VSCode clients hitting AnythingLLM
      - CORS_ALLOW_ORIGIN=*
      - DISABLE_TELEMETRY=true
    volumes:
      - ./storage:/app/server/storage
    depends_on:
      - ollama
      - qdrant
    restart: unless-stopped

volumes:
  qdrant_storage:
  ollama_models:      